# AWS para Engenharia de Dados - Conceitos Fundamentais ‚òÅÔ∏è

Guia completo dos principais servi√ßos AWS para Engenheiros de Dados, cobrindo armazenamento, processamento, seguran√ßa e orquestra√ß√£o de dados.

## üìã √çndice

1. [Vis√£o Geral da AWS para Dados](#vis√£o-geral)
2. [Amazon S3 - Simple Storage Service](#amazon-s3)
3. [AWS KMS - Key Management Service](#aws-kms)
4. [Servi√ßos de Processamento](#servi√ßos-de-processamento)
5. [Servi√ßos de Analytics](#servi√ßos-de-analytics)
6. [Orquestra√ß√£o e Workflow](#orquestra√ß√£o-e-workflow)
7. [Monitoramento e Logging](#monitoramento-e-logging)
8. [Seguran√ßa e Compliance](#seguran√ßa-e-compliance)
9. [Arquiteturas de Refer√™ncia](#arquiteturas-de-refer√™ncia)
10. [Boas Pr√°ticas](#boas-pr√°ticas)

---

## üåê Vis√£o Geral da AWS para Dados {#vis√£o-geral}

### Pilares da Engenharia de Dados na AWS

1. **Ingest√£o**: Kinesis, DMS, DataSync
2. **Armazenamento**: S3, EFS, EBS
3. **Processamento**: EMR, Glue, Lambda, Batch
4. **Analytics**: Redshift, Athena, QuickSight
5. **Orquestra√ß√£o**: Step Functions, Airflow (MWAA)
6. **Seguran√ßa**: IAM, KMS, VPC
7. **Monitoramento**: CloudWatch, CloudTrail

### Modelo de Responsabilidade Compartilhada

**AWS Respons√°vel por:**
- Infraestrutura f√≠sica
- Seguran√ßa da nuvem
- Patches do sistema operacional
- Configura√ß√£o de rede

**Cliente Respons√°vel por:**
- Dados e criptografia
- Configura√ß√£o de seguran√ßa
- Gerenciamento de identidade
- Configura√ß√£o de aplica√ß√µes

---

## ü™£ Amazon S3 - Simple Storage Service {#amazon-s3}

### Conceitos Fundamentais

**S3** √© um servi√ßo de armazenamento de objetos altamente escal√°vel, dur√°vel e dispon√≠vel.

#### Hierarquia S3
```
Bucket (Cont√™iner global √∫nico)
‚îú‚îÄ‚îÄ Prefix/Folder (Organiza√ß√£o l√≥gica)
‚îÇ   ‚îú‚îÄ‚îÄ Object (Arquivo + Metadata)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Key (Nome √∫nico)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Version ID
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Metadata
```

### Classes de Armazenamento S3

#### 1. **S3 Standard**
- **Uso**: Dados acessados frequentemente
- **Durabilidade**: 99.999999999% (11 9's)
- **Disponibilidade**: 99.99%
- **Lat√™ncia**: Milissegundos
- **Custo**: Mais alto para armazenamento, menor para acesso

#### 2. **S3 Intelligent-Tiering**
- **Uso**: Padr√µes de acesso desconhecidos ou vari√°veis
- **Funcionalidade**: Move automaticamente entre tiers
- **Tiers**:
  - Frequent Access
  - Infrequent Access
  - Archive Instant Access
  - Archive Access
  - Deep Archive Access

#### 3. **S3 Standard-IA (Infrequent Access)**
- **Uso**: Dados acessados menos frequentemente
- **Disponibilidade**: 99.9%
- **Custo**: Menor armazenamento, maior acesso
- **Tempo m√≠nimo**: 30 dias

#### 4. **S3 One Zone-IA**
- **Uso**: Dados n√£o cr√≠ticos, acessados raramente
- **Disponibilidade**: 99.5% (uma AZ)
- **Custo**: 20% menor que Standard-IA
- **Risco**: Perda se AZ falhar

#### 5. **S3 Glacier Instant Retrieval**
- **Uso**: Arquivamento com acesso instant√¢neo
- **Retrieval**: Milissegundos
- **Tempo m√≠nimo**: 90 dias
- **Custo**: Muito baixo para armazenamento

#### 6. **S3 Glacier Flexible Retrieval**
- **Uso**: Backup e arquivamento
- **Retrieval**: 1-12 horas
- **Tempo m√≠nimo**: 90 dias
- **Op√ß√µes**: Expedited (1-5 min), Standard (3-5h), Bulk (5-12h)

#### 7. **S3 Glacier Deep Archive**
- **Uso**: Arquivamento de longo prazo
- **Retrieval**: 12-48 horas
- **Tempo m√≠nimo**: 180 dias
- **Custo**: Mais baixo da AWS

### Lifecycle Policies

```json
{
  "Rules": [
    {
      "ID": "DataLakeLifecycle",
      "Status": "Enabled",
      "Filter": {
        "Prefix": "data/"
      },
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "STANDARD_IA"
        },
        {
          "Days": 90,
          "StorageClass": "GLACIER"
        },
        {
          "Days": 365,
          "StorageClass": "DEEP_ARCHIVE"
        }
      ]
    }
  ]
}
```

### Recursos Avan√ßados do S3

#### Versionamento
- Mant√©m m√∫ltiplas vers√µes do mesmo objeto
- Prote√ß√£o contra exclus√£o acidental
- Integra√ß√£o com MFA Delete

#### Cross-Region Replication (CRR)
- Replica√ß√£o autom√°tica entre regi√µes
- Compliance e disaster recovery
- Redu√ß√£o de lat√™ncia

#### Transfer Acceleration
- Usa CloudFront edge locations
- Acelera uploads para S3
- Ideal para uploads globais

#### Multipart Upload
- Upload de arquivos grandes (>100MB)
- Paraleliza√ß√£o e resumo
- Melhora performance e confiabilidade

### S3 para Data Lakes

#### Estrutura Recomendada
```
data-lake-bucket/
‚îú‚îÄ‚îÄ raw/                    # Dados brutos
‚îÇ   ‚îú‚îÄ‚îÄ year=2024/
‚îÇ   ‚îú‚îÄ‚îÄ month=01/
‚îÇ   ‚îî‚îÄ‚îÄ day=15/
‚îú‚îÄ‚îÄ processed/              # Dados processados
‚îÇ   ‚îú‚îÄ‚îÄ bronze/            # Limpeza b√°sica
‚îÇ   ‚îú‚îÄ‚îÄ silver/            # Transforma√ß√µes
‚îÇ   ‚îî‚îÄ‚îÄ gold/              # Dados anal√≠ticos
‚îú‚îÄ‚îÄ archive/               # Dados arquivados
‚îî‚îÄ‚îÄ temp/                  # Dados tempor√°rios
```

#### Particionamento
```
# Por data (Hive-style)
s3://bucket/table/year=2024/month=01/day=15/

# Por regi√£o
s3://bucket/sales/region=us-east/year=2024/

# M√∫ltiplas dimens√µes
s3://bucket/events/year=2024/month=01/day=15/hour=14/
```

---

## üîê AWS KMS - Key Management Service {#aws-kms}

### Conceitos Fundamentais

**KMS** √© um servi√ßo gerenciado para cria√ß√£o e controle de chaves de criptografia.

#### Tipos de Chaves

1. **AWS Managed Keys**
   - Criadas e gerenciadas pela AWS
   - Rota√ß√£o autom√°tica anual
   - Sem custo adicional
   - Formato: `aws/service-name`

2. **Customer Managed Keys (CMK)**
   - Criadas e gerenciadas pelo cliente
   - Controle total sobre pol√≠ticas
   - Rota√ß√£o opcional
   - Custo: $1/m√™s por chave

3. **AWS Owned Keys**
   - Propriedade da AWS
   - Usadas internamente
   - N√£o vis√≠veis ao cliente

#### Tipos de Chaves por Material

1. **KMS Keys (HSM)**
   - Material gerado no HSM da AWS
   - FIPS 140-2 Level 2
   - Mais comum e econ√¥mico

2. **CloudHSM Keys**
   - Material gerado no CloudHSM
   - FIPS 140-2 Level 3
   - Controle exclusivo do HSM

3. **External Keys (BYOK)**
   - Material importado pelo cliente
   - Controle total sobre o material
   - Responsabilidade de backup

### Criptografia no S3 com KMS

#### Server-Side Encryption (SSE)

1. **SSE-S3**
   - Chaves gerenciadas pelo S3
   - AES-256
   - Transparente para o usu√°rio

2. **SSE-KMS**
   - Chaves gerenciadas pelo KMS
   - Auditoria via CloudTrail
   - Controle granular de acesso

3. **SSE-C**
   - Chaves fornecidas pelo cliente
   - Cliente gerencia chaves
   - AWS n√£o armazena chaves

#### Exemplo de Pol√≠tica KMS
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowDataEngineers",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:role/DataEngineerRole"
      },
      "Action": [
        "kms:Encrypt",
        "kms:Decrypt",
        "kms:ReEncrypt*",
        "kms:GenerateDataKey*",
        "kms:DescribeKey"
      ],
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "kms:ViaService": "s3.us-east-1.amazonaws.com"
        }
      }
    }
  ]
}
```

### Envelope Encryption

```
1. KMS gera Data Encryption Key (DEK)
2. DEK criptografa os dados
3. KMS criptografa DEK com CMK
4. Armazena DEK criptografada com dados
5. Para descriptografar:
   - KMS descriptografa DEK
   - DEK descriptografa dados
```

---

## ‚öôÔ∏è Servi√ßos de Processamento {#servi√ßos-de-processamento}

### AWS Glue

#### Componentes Principais

1. **Data Catalog**
   - Metastore centralizado
   - Schema discovery autom√°tico
   - Integra√ß√£o com Athena, EMR, Redshift

2. **ETL Jobs**
   - Spark-based transformations
   - Python ou Scala
   - Serverless execution

3. **Crawlers**
   - Descoberta autom√°tica de schema
   - Suporte para m√∫ltiplos formatos
   - Agendamento autom√°tico

4. **DataBrew**
   - Visual data preparation
   - No-code transformations
   - Profile e quality rules

#### Exemplo Glue Job
```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Read from Data Catalog
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="sales_db",
    table_name="raw_sales"
)

# Transform data
transformed = ApplyMapping.apply(
    frame=datasource,
    mappings=[
        ("customer_id", "string", "customer_id", "string"),
        ("amount", "double", "amount", "double"),
        ("date", "string", "sale_date", "timestamp")
    ]
)

# Write to S3
glueContext.write_dynamic_frame.from_options(
    frame=transformed,
    connection_type="s3",
    connection_options={
        "path": "s3://processed-data/sales/",
        "partitionKeys": ["year", "month"]
    },
    format="parquet"
)

job.commit()
```

### Amazon EMR

#### Componentes
- **Master Node**: Coordena cluster
- **Core Nodes**: HDFS e processamento
- **Task Nodes**: Apenas processamento

#### Configura√ß√µes

1. **EMR on EC2**
   - Controle total sobre inst√¢ncias
   - Customiza√ß√£o completa
   - Gerenciamento manual

2. **EMR on EKS**
   - Kubernetes-based
   - Melhor isolamento
   - Integra√ß√£o com EKS

3. **EMR Serverless**
   - Sem gerenciamento de infraestrutura
   - Auto-scaling autom√°tico
   - Pay-per-use

#### Exemplo EMR Step
```json
{
  "Name": "Spark ETL Job",
  "ActionOnFailure": "TERMINATE_CLUSTER",
  "HadoopJarStep": {
    "Jar": "command-runner.jar",
    "Args": [
      "spark-submit",
      "--deploy-mode", "cluster",
      "--class", "com.company.DataProcessor",
      "s3://my-bucket/jars/data-processor.jar",
      "s3://input-bucket/data/",
      "s3://output-bucket/processed/"
    ]
  }
}
```

### AWS Lambda

#### Casos de Uso para Dados
- Triggers para S3 events
- Processamento de streaming
- APIs para dados
- Orquestra√ß√£o simples

#### Exemplo Lambda para S3
```python
import json
import boto3
import pandas as pd
from io import StringIO

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    
    # Get S3 event details
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    # Read CSV from S3
    obj = s3.get_object(Bucket=bucket, Key=key)
    df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))
    
    # Simple transformation
    df['processed_date'] = pd.Timestamp.now()
    df['amount_usd'] = df['amount'] * df['exchange_rate']
    
    # Write back to S3
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False)
    
    s3.put_object(
        Bucket='processed-bucket',
        Key=f'processed/{key}',
        Body=csv_buffer.getvalue()
    )
    
    return {
        'statusCode': 200,
        'body': json.dumps(f'Processed {len(df)} records')
    }
```

---

## üìä Servi√ßos de Analytics {#servi√ßos-de-analytics}

### Amazon Athena

#### Caracter√≠sticas
- **Serverless**: Sem infraestrutura para gerenciar
- **SQL Standard**: ANSI SQL compat√≠vel
- **Pay-per-query**: Cobra por dados escaneados
- **Integra√ß√£o**: S3, Glue Data Catalog

#### Otimiza√ß√µes

1. **Particionamento**
```sql
-- Criar tabela particionada
CREATE TABLE sales_partitioned (
  customer_id string,
  amount double,
  product_id string
)
PARTITIONED BY (
  year int,
  month int,
  day int
)
STORED AS PARQUET
LOCATION 's3://my-bucket/sales/'

-- Query otimizada
SELECT customer_id, SUM(amount)
FROM sales_partitioned
WHERE year = 2024 AND month = 1
GROUP BY customer_id
```

2. **Columnar Formats**
```sql
-- Converter para Parquet
CREATE TABLE sales_parquet
WITH (
  format = 'PARQUET',
  external_location = 's3://my-bucket/sales-parquet/'
)
AS SELECT * FROM sales_csv
```

3. **Compression**
```sql
-- Usar compress√£o
CREATE TABLE sales_compressed
WITH (
  format = 'PARQUET',
  parquet_compression = 'SNAPPY'
)
AS SELECT * FROM sales
```

### Amazon Redshift

#### Arquitetura
- **Leader Node**: Query planning e coordination
- **Compute Nodes**: Data storage e processing
- **Node Slices**: Parallel processing units

#### Tipos de Cluster

1. **Provisioned**
   - Inst√¢ncias dedicadas
   - Controle sobre configura√ß√£o
   - Previsibilidade de custos

2. **Serverless**
   - Auto-scaling autom√°tico
   - Pay-per-use
   - Ideal para workloads vari√°veis

#### Distribution Styles

```sql
-- EVEN distribution
CREATE TABLE sales (
  sale_id INT,
  customer_id INT,
  amount DECIMAL(10,2)
)
DISTSTYLE EVEN;

-- KEY distribution
CREATE TABLE customers (
  customer_id INT,
  name VARCHAR(100)
)
DISTSTYLE KEY
DISTKEY (customer_id);

-- ALL distribution
CREATE TABLE products (
  product_id INT,
  name VARCHAR(100),
  category VARCHAR(50)
)
DISTSTYLE ALL;
```

#### Sort Keys

```sql
-- Compound sort key
CREATE TABLE events (
  event_id BIGINT,
  user_id INT,
  event_time TIMESTAMP,
  event_type VARCHAR(50)
)
COMPOUND SORTKEY (event_time, user_id);

-- Interleaved sort key
CREATE TABLE logs (
  log_id BIGINT,
  timestamp TIMESTAMP,
  level VARCHAR(10),
  message TEXT
)
INTERLEAVED SORTKEY (timestamp, level);
```

### Amazon QuickSight

#### Caracter√≠sticas
- **Serverless BI**: Sem infraestrutura
- **ML Insights**: Anomaly detection, forecasting
- **Embedded Analytics**: Integra√ß√£o em aplica√ß√µes
- **Pay-per-session**: Modelo de pre√ßos flex√≠vel

#### Data Sources
- S3, Athena, Redshift
- RDS, Aurora
- SaaS applications
- On-premises databases

---

## üîÑ Orquestra√ß√£o e Workflow {#orquestra√ß√£o-e-workflow}

### AWS Step Functions

#### State Types

1. **Task**: Executa trabalho
2. **Choice**: Decis√µes condicionais
3. **Parallel**: Execu√ß√£o paralela
4. **Map**: Itera√ß√£o sobre arrays
5. **Wait**: Delay temporal
6. **Pass**: Transforma√ß√£o de dados
7. **Fail/Succeed**: Estados terminais

#### Exemplo ETL Workflow
```json
{
  "Comment": "ETL Pipeline",
  "StartAt": "ValidateInput",
  "States": {
    "ValidateInput": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:us-east-1:123456789012:function:ValidateData",
      "Next": "ProcessData",
      "Catch": [{
        "ErrorEquals": ["ValidationError"],
        "Next": "HandleError"
      }]
    },
    "ProcessData": {
      "Type": "Parallel",
      "Branches": [
        {
          "StartAt": "TransformCustomers",
          "States": {
            "TransformCustomers": {
              "Type": "Task",
              "Resource": "arn:aws:states:::glue:startJobRun.sync",
              "Parameters": {
                "JobName": "customer-etl"
              },
              "End": true
            }
          }
        },
        {
          "StartAt": "TransformOrders",
          "States": {
            "TransformOrders": {
              "Type": "Task",
              "Resource": "arn:aws:states:::glue:startJobRun.sync",
              "Parameters": {
                "JobName": "orders-etl"
              },
              "End": true
            }
          }
        }
      ],
      "Next": "LoadToWarehouse"
    },
    "LoadToWarehouse": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:us-east-1:123456789012:function:LoadToRedshift",
      "End": true
    },
    "HandleError": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:us-east-1:123456789012:function:SendAlert",
      "End": true
    }
  }
}
```

### Amazon MWAA (Managed Apache Airflow)

#### Caracter√≠sticas
- **Managed Airflow**: Sem gerenciamento de infraestrutura
- **Auto-scaling**: Workers autom√°ticos
- **Security**: VPC, IAM integration
- **Monitoring**: CloudWatch integration

#### Exemplo DAG
```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.amazon.aws.operators.glue import GlueJobOperator
from airflow.providers.amazon.aws.operators.s3 import S3KeySensor
from airflow.providers.amazon.aws.operators.lambda_function import LambdaInvokeFunctionOperator

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'daily_etl_pipeline',
    default_args=default_args,
    description='Daily ETL Pipeline',
    schedule_interval='0 2 * * *',  # 2 AM daily
    catchup=False
)

# Wait for input file
wait_for_file = S3KeySensor(
    task_id='wait_for_input_file',
    bucket_name='input-bucket',
    bucket_key='data/{{ ds }}/sales.csv',
    timeout=3600,
    poke_interval=300,
    dag=dag
)

# Validate data
validate_data = LambdaInvokeFunctionOperator(
    task_id='validate_data',
    function_name='validate-sales-data',
    payload='{"date": "{{ ds }}"}',
    dag=dag
)

# Transform data
transform_data = GlueJobOperator(
    task_id='transform_sales_data',
    job_name='sales-etl-job',
    script_args={
        '--input_path': 's3://input-bucket/data/{{ ds }}/',
        '--output_path': 's3://processed-bucket/sales/{{ ds }}/',
        '--date': '{{ ds }}'
    },
    dag=dag
)

# Load to warehouse
load_to_warehouse = LambdaInvokeFunctionOperator(
    task_id='load_to_redshift',
    function_name='load-sales-to-redshift',
    payload='{"date": "{{ ds }}"}',
    dag=dag
)

# Set dependencies
wait_for_file >> validate_data >> transform_data >> load_to_warehouse
```

---

## üìà Monitoramento e Logging {#monitoramento-e-logging}

### Amazon CloudWatch

#### M√©tricas Importantes

1. **S3 Metrics**
   - BucketSizeBytes
   - NumberOfObjects
   - AllRequests
   - GetRequests
   - PutRequests

2. **Glue Metrics**
   - glue.driver.aggregate.numCompletedTasks
   - glue.driver.aggregate.numFailedTasks
   - glue.driver.BlockManager.disk.diskSpaceUsed_MB

3. **EMR Metrics**
   - IsIdle
   - CoreNodesRunning
   - AppsCompleted
   - AppsFailed

#### Custom Metrics
```python
import boto3
from datetime import datetime

cloudwatch = boto3.client('cloudwatch')

# Send custom metric
cloudwatch.put_metric_data(
    Namespace='DataPipeline/ETL',
    MetricData=[
        {
            'MetricName': 'RecordsProcessed',
            'Value': 1000000,
            'Unit': 'Count',
            'Timestamp': datetime.utcnow(),
            'Dimensions': [
                {
                    'Name': 'Pipeline',
                    'Value': 'sales-etl'
                },
                {
                    'Name': 'Environment',
                    'Value': 'production'
                }
            ]
        }
    ]
)
```

#### CloudWatch Alarms
```json
{
  "AlarmName": "ETL-Job-Failure",
  "AlarmDescription": "Alert when ETL job fails",
  "MetricName": "Errors",
  "Namespace": "AWS/Glue",
  "Statistic": "Sum",
  "Period": 300,
  "EvaluationPeriods": 1,
  "Threshold": 1,
  "ComparisonOperator": "GreaterThanOrEqualToThreshold",
  "Dimensions": [
    {
      "Name": "JobName",
      "Value": "sales-etl-job"
    }
  ],
  "AlarmActions": [
    "arn:aws:sns:us-east-1:123456789012:data-alerts"
  ]
}
```

### AWS CloudTrail

#### Eventos Importantes
- API calls para S3, Glue, EMR
- Mudan√ßas em pol√≠ticas IAM
- Acesso a dados sens√≠veis
- Modifica√ß√µes em recursos

#### Exemplo Query CloudTrail
```sql
-- Athena query para CloudTrail logs
SELECT 
    eventtime,
    eventname,
    sourceipaddress,
    useragent,
    errorcode,
    errormessage
FROM cloudtrail_logs
WHERE 
    eventtime >= '2024-01-01'
    AND eventname LIKE '%S3%'
    AND errorcode IS NOT NULL
ORDER BY eventtime DESC
```

---

## üîí Seguran√ßa e Compliance {#seguran√ßa-e-compliance}

### AWS IAM (Identity and Access Management)

#### Princ√≠pios de Seguran√ßa

1. **Least Privilege**: M√≠nimas permiss√µes necess√°rias
2. **Defense in Depth**: M√∫ltiplas camadas de seguran√ßa
3. **Zero Trust**: Verificar sempre, nunca confiar
4. **Separation of Duties**: Dividir responsabilidades

#### Exemplo Pol√≠tica IAM para Data Engineer
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "S3DataAccess",
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::data-lake-bucket/*",
        "arn:aws:s3:::data-lake-bucket"
      ],
      "Condition": {
        "StringEquals": {
          "s3:x-amz-server-side-encryption": "aws:kms"
        }
      }
    },
    {
      "Sid": "GlueJobAccess",
      "Effect": "Allow",
      "Action": [
        "glue:StartJobRun",
        "glue:GetJobRun",
        "glue:GetJobRuns",
        "glue:BatchStopJobRun"
      ],
      "Resource": "arn:aws:glue:*:*:job/data-*"
    },
    {
      "Sid": "KMSAccess",
      "Effect": "Allow",
      "Action": [
        "kms:Decrypt",
        "kms:GenerateDataKey"
      ],
      "Resource": "arn:aws:kms:*:*:key/12345678-1234-1234-1234-123456789012",
      "Condition": {
        "StringEquals": {
          "kms:ViaService": [
            "s3.us-east-1.amazonaws.com",
            "glue.us-east-1.amazonaws.com"
          ]
        }
      }
    }
  ]
}
```

### Data Classification

#### N√≠veis de Classifica√ß√£o

1. **Public**: Dados p√∫blicos
2. **Internal**: Uso interno da empresa
3. **Confidential**: Dados sens√≠veis
4. **Restricted**: Dados altamente sens√≠veis

#### Implementa√ß√£o com Tags
```json
{
  "TagSet": [
    {
      "Key": "DataClassification",
      "Value": "Confidential"
    },
    {
      "Key": "DataOwner",
      "Value": "finance-team"
    },
    {
      "Key": "RetentionPeriod",
      "Value": "7years"
    },
    {
      "Key": "ComplianceRequirement",
      "Value": "SOX"
    }
  ]
}
```

### AWS Macie

#### Funcionalidades
- **Data Discovery**: Encontra dados sens√≠veis
- **Classification**: Classifica automaticamente
- **Monitoring**: Monitora acesso a dados
- **Alerting**: Alertas de seguran√ßa

#### Tipos de Dados Detectados
- PII (Personally Identifiable Information)
- PHI (Protected Health Information)
- Financial data
- Credentials e tokens

---

## üèóÔ∏è Arquiteturas de Refer√™ncia {#arquiteturas-de-refer√™ncia}

### Data Lake Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Data Sources  ‚îÇ    ‚îÇ   Ingestion     ‚îÇ    ‚îÇ   Storage       ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Databases     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ‚Ä¢ Kinesis       ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ‚Ä¢ S3 Raw        ‚îÇ
‚îÇ ‚Ä¢ APIs          ‚îÇ    ‚îÇ ‚Ä¢ DMS           ‚îÇ    ‚îÇ ‚Ä¢ S3 Processed  ‚îÇ
‚îÇ ‚Ä¢ Files         ‚îÇ    ‚îÇ ‚Ä¢ Lambda        ‚îÇ    ‚îÇ ‚Ä¢ S3 Curated    ‚îÇ
‚îÇ ‚Ä¢ Streaming     ‚îÇ    ‚îÇ ‚Ä¢ Glue          ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Consumption   ‚îÇ    ‚îÇ   Analytics     ‚îÇ    ‚îÇ   Processing    ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ QuickSight    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚Ä¢ Athena        ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚Ä¢ Glue ETL      ‚îÇ
‚îÇ ‚Ä¢ Tableau       ‚îÇ    ‚îÇ ‚Ä¢ Redshift      ‚îÇ    ‚îÇ ‚Ä¢ EMR           ‚îÇ
‚îÇ ‚Ä¢ Applications  ‚îÇ    ‚îÇ ‚Ä¢ SageMaker     ‚îÇ    ‚îÇ ‚Ä¢ Lambda        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Modern Data Stack

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Governance Layer                        ‚îÇ
‚îÇ  ‚Ä¢ IAM ‚Ä¢ KMS ‚Ä¢ CloudTrail ‚Ä¢ Macie ‚Ä¢ Lake Formation         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Streaming     ‚îÇ    ‚îÇ   Batch Processing ‚îÇ    ‚îÇ   Real-time     ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                    ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Kinesis       ‚îÇ    ‚îÇ ‚Ä¢ Glue             ‚îÇ    ‚îÇ ‚Ä¢ Lambda        ‚îÇ
‚îÇ ‚Ä¢ MSK           ‚îÇ    ‚îÇ ‚Ä¢ EMR              ‚îÇ    ‚îÇ ‚Ä¢ Kinesis       ‚îÇ
‚îÇ ‚Ä¢ Kinesis       ‚îÇ    ‚îÇ ‚Ä¢ Step Functions   ‚îÇ    ‚îÇ ‚Ä¢ Analytics     ‚îÇ
‚îÇ   Analytics     ‚îÇ    ‚îÇ ‚Ä¢ MWAA             ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ     Data Lake (S3)    ‚îÇ
                    ‚îÇ                       ‚îÇ
                    ‚îÇ ‚Ä¢ Bronze (Raw)        ‚îÇ
                    ‚îÇ ‚Ä¢ Silver (Cleaned)    ‚îÇ
                    ‚îÇ ‚Ä¢ Gold (Curated)      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Analytics Layer     ‚îÇ
                    ‚îÇ                       ‚îÇ
                    ‚îÇ ‚Ä¢ Athena              ‚îÇ
                    ‚îÇ ‚Ä¢ Redshift            ‚îÇ
                    ‚îÇ ‚Ä¢ QuickSight          ‚îÇ
                    ‚îÇ ‚Ä¢ SageMaker           ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Lambda Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Data Sources   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Speed Layer    ‚îÇ    ‚îÇ  Batch Layer    ‚îÇ    ‚îÇ  Serving Layer  ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Kinesis       ‚îÇ    ‚îÇ ‚Ä¢ S3            ‚îÇ    ‚îÇ ‚Ä¢ DynamoDB      ‚îÇ
‚îÇ ‚Ä¢ Lambda        ‚îÇ    ‚îÇ ‚Ä¢ Glue          ‚îÇ    ‚îÇ ‚Ä¢ ElastiCache   ‚îÇ
‚îÇ ‚Ä¢ Real-time     ‚îÇ    ‚îÇ ‚Ä¢ EMR           ‚îÇ    ‚îÇ ‚Ä¢ API Gateway   ‚îÇ
‚îÇ   Processing    ‚îÇ    ‚îÇ ‚Ä¢ Historical    ‚îÇ    ‚îÇ ‚Ä¢ Applications  ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ   Processing    ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                      ‚îÇ                      ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ     Batch Views         ‚îÇ
                    ‚îÇ   (Precomputed)         ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚úÖ Boas Pr√°ticas {#boas-pr√°ticas}

### Seguran√ßa

1. **Encryption Everywhere**
   - Dados em tr√¢nsito (TLS/SSL)
   - Dados em repouso (KMS)
   - Chaves rotacionadas regularmente

2. **Access Control**
   - Princ√≠pio do menor privil√©gio
   - MFA para acesso administrativo
   - Auditoria de acessos

3. **Network Security**
   - VPC endpoints para servi√ßos AWS
   - Security groups restritivos
   - NACLs quando necess√°rio

### Performance

1. **S3 Optimization**
   - Usar formatos colunares (Parquet, ORC)
   - Implementar particionamento
   - Configurar lifecycle policies
   - Usar Transfer Acceleration

2. **Query Optimization**
   - Particionar dados por colunas de filtro
   - Usar compress√£o adequada
   - Limitar escaneamento de dados
   - Implementar caching

3. **Cost Optimization**
   - Monitorar custos regularmente
   - Usar Spot instances para EMR
   - Implementar auto-scaling
   - Arquivar dados antigos

### Governan√ßa

1. **Data Catalog**
   - Documentar todos os datasets
   - Manter metadados atualizados
   - Implementar data lineage
   - Definir data owners

2. **Quality Assurance**
   - Valida√ß√£o de dados na ingest√£o
   - Monitoramento de qualidade
   - Alertas para anomalias
   - Testes automatizados

3. **Compliance**
   - Classificar dados por sensibilidade
   - Implementar reten√ß√£o de dados
   - Auditoria de acessos
   - Documentar processos

### Monitoramento

1. **Observability**
   - Logs estruturados
   - M√©tricas customizadas
   - Distributed tracing
   - Alertas proativos

2. **Performance Monitoring**
   - Lat√™ncia de queries
   - Throughput de pipelines
   - Utiliza√ß√£o de recursos
   - Custos por workload

### Disaster Recovery

1. **Backup Strategy**
   - Cross-region replication
   - Point-in-time recovery
   - Backup automatizado
   - Testes de restore

2. **High Availability**
   - Multi-AZ deployments
   - Auto-scaling groups
   - Health checks
   - Failover autom√°tico

---

## üìö Recursos Adicionais

### Documenta√ß√£o Oficial
- [AWS Data Analytics](https://aws.amazon.com/big-data/datalakes-and-analytics/)
- [AWS Well-Architected Framework](https://aws.amazon.com/architecture/well-architected/)
- [AWS Security Best Practices](https://aws.amazon.com/security/)

### Certifica√ß√µes Relevantes
- **AWS Certified Data Engineer - Associate**
- **AWS Certified Solutions Architect**
- **AWS Certified Security - Specialty**
- **AWS Certified Machine Learning - Specialty**

### Ferramentas de Terceiros
- **dbt**: Transforma√ß√µes SQL
- **Apache Airflow**: Orquestra√ß√£o
- **Terraform**: Infrastructure as Code
- **DataDog**: Monitoramento

---

## üéØ Conclus√£o

A AWS oferece um ecossistema completo para Engenharia de Dados, desde ingest√£o at√© visualiza√ß√£o. O sucesso depende de:

1. **Arquitetura bem planejada**
2. **Seguran√ßa desde o design**
3. **Otimiza√ß√£o cont√≠nua**
4. **Monitoramento proativo**
5. **Governan√ßa de dados**

Com esses conceitos e pr√°ticas, voc√™ estar√° preparado para construir solu√ß√µes robustas e escal√°veis de dados na AWS! üöÄ

---

**Happy Data Engineering! ‚òÅÔ∏èüìä**